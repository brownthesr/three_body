{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_orbit, random_config\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchdiffeq import odeint\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 500\n",
    "state = random_config(nu=2e-1, min_radius=0.9, max_radius=1.8)\n",
    "orbit, settings = get_orbit(state, t_points=num_steps, t_span=[0,15], nbodies=3)\n",
    "state = state[:,1:]\n",
    "orbit = orbit[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6709, -0.7914,  0.5092,  0.4211],\n",
      "        [ 0.3500,  0.9767, -0.8044,  0.2540],\n",
      "        [-1.0208, -0.1853,  0.1512, -0.7419]])\n",
      "tensor([[[ 0.6709,  0.6860,  0.7009,  ...,  1.1561,  1.1514,  1.1464],\n",
      "         [-0.7914, -0.7786, -0.7654,  ..., -0.0260, -0.0080,  0.0099],\n",
      "         [ 0.5092,  0.4987,  0.4878,  ..., -0.1540, -0.1609, -0.1678],\n",
      "         [ 0.4211,  0.4332,  0.4452,  ...,  0.5985,  0.5975,  0.5963]],\n",
      "\n",
      "        [[ 0.3500,  0.3257,  0.3013,  ..., -1.6417, -1.6268, -1.6119],\n",
      "         [ 0.9767,  0.9841,  0.9911,  ..., -1.0634, -1.0867, -1.1093],\n",
      "         [-0.8044, -0.8097, -0.8147,  ...,  0.4930,  0.4954,  0.4973],\n",
      "         [ 0.2540,  0.2388,  0.2234,  ..., -0.7870, -0.7638, -0.7415]],\n",
      "\n",
      "        [[-1.0208, -1.0161, -1.0108,  ..., -1.6670, -1.6814, -1.6957],\n",
      "         [-0.1853, -0.2075, -0.2297,  ...,  0.0913,  0.0946,  0.0973],\n",
      "         [ 0.1512,  0.1670,  0.1828,  ..., -0.4831, -0.4786, -0.4737],\n",
      "         [-0.7419, -0.7388, -0.7354,  ...,  0.1216,  0.0995,  0.0784]]])\n",
      "torch.Size([3, 4, 500])\n",
      "tensor([ 0.6709, -0.7914,  0.3500,  0.9767, -1.0208, -0.1853,  0.5092,  0.4211,\n",
      "        -0.8044,  0.2540,  0.1512, -0.7419])\n",
      "tensor([[ 0.6709,  0.6860,  0.7009,  ...,  1.1561,  1.1514,  1.1464],\n",
      "        [-0.7914, -0.7786, -0.7654,  ..., -0.0260, -0.0080,  0.0099],\n",
      "        [ 0.3500,  0.3257,  0.3013,  ..., -1.6417, -1.6268, -1.6119],\n",
      "        ...,\n",
      "        [ 0.2540,  0.2388,  0.2234,  ..., -0.7870, -0.7638, -0.7415],\n",
      "        [ 0.1512,  0.1670,  0.1828,  ..., -0.4831, -0.4786, -0.4737],\n",
      "        [-0.7419, -0.7388, -0.7354,  ...,  0.1216,  0.0995,  0.0784]])\n"
     ]
    }
   ],
   "source": [
    "state_tensor = torch.Tensor(state).to(device)\n",
    "orbit_tensor = torch.Tensor(orbit).to(device)\n",
    "new_state = torch.cat((torch.flatten(state_tensor[:,:2]), torch.flatten(state_tensor[:,2:])))\n",
    "new_orbit = torch.cat((torch.flatten(orbit_tensor[:,:2], start_dim = 0, end_dim = 1), torch.flatten(orbit_tensor[:,2:], start_dim = 0, end_dim = 1)), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.odefunc = mlp(\n",
    "            input_dim = 6,\n",
    "            hidden_dim = 64,\n",
    "            output_dim = 6,\n",
    "            hidden_depth = 2\n",
    "        )\n",
    "\n",
    "        def init(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                std = 1.0 / np.sqrt(m.weight.size(1))\n",
    "                m.weight.data.uniform_(-2.0 * std, 2.0 * std)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        self.odefunc.apply(init)\n",
    "\n",
    "    def forward(self, t, state):\n",
    "        state_reshaped = torch.flatten(state[:,:2], start_dim = -2)\n",
    "        acceleration = self.odefunc(state_reshaped).view(3, 2)\n",
    "        acceleration = acceleration - acceleration.mean(dim = -2, keepdim = True)\n",
    "        return torch.cat((state[:,2:], acceleration), dim = -1)\n",
    "\n",
    "    def simulate(self, state, times):\n",
    "        solution = odeint(self, state, times, atol=1e-8, rtol=1e-8, method=\"dopri5\", options = {\"dtype\": torch.float32})\n",
    "        # B, S, T\n",
    "        return solution\n",
    "\n",
    "def mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None, act=nn.ReLU):\n",
    "    if hidden_depth == 0:\n",
    "        mods = [nn.Linear(input_dim, output_dim)]\n",
    "    else:\n",
    "        mods = [nn.Linear(input_dim, hidden_dim), act()]\n",
    "        for i in range(hidden_depth - 1):\n",
    "            mods += [nn.Linear(hidden_dim, hidden_dim), act()]\n",
    "        mods.append(nn.Linear(hidden_dim, output_dim))\n",
    "    if output_mod is not None:\n",
    "        mods.append(output_mod)\n",
    "    trunk = nn.Sequential(*mods)\n",
    "    return trunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetricNeuralODE(nn.Module):\n",
    "    def __init__(self, hidden_dim, hidden_depth):\n",
    "        super().__init__()\n",
    "        self.odefunc = mlp(\n",
    "            input_dim = 6,\n",
    "            hidden_dim = hidden_dim,\n",
    "            output_dim = 2,\n",
    "            hidden_depth = hidden_depth\n",
    "        )\n",
    "        self.input_weight = torch.Tensor([\n",
    "    [\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1]\n",
    "\n",
    "    ],\n",
    "    [\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0]\n",
    "    ]\n",
    "        ]).float()\n",
    "        self.output_weight = torch.Tensor([\n",
    "    [2, 2, -1, -1, -1, -1],\n",
    "    [-1, -1, 2, 2, -1, -1],\n",
    "    [-1, -1, -1, -1, 2, 2]\n",
    "        ]).float()\n",
    "\n",
    "        def init(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                std = 1.0 / np.sqrt(m.weight.size(1))\n",
    "                m.weight.data.uniform_(-2.0 * std, 2.0 * std)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        self.odefunc.apply(init)\n",
    "\n",
    "    def forward(self, t, state):\n",
    "        return torch.cat((state[6:], (self.output_weight @ self.odefunc(self.input_weight @ state[:6])).view(-1)))\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def simulate(self, state, times):\n",
    "        solution = odeint(self, state, times, atol=1e-8, rtol=1e-8, method=\"dopri5\", options = {\"dtype\": torch.float32})\n",
    "        # B, S, T\n",
    "        return solution\n",
    "\n",
    "def mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None, act=nn.ReLU):\n",
    "    if hidden_depth == 0:\n",
    "        mods = [nn.Linear(input_dim, output_dim)]\n",
    "    else:\n",
    "        mods = [nn.Linear(input_dim, hidden_dim), act()]\n",
    "        for i in range(hidden_depth - 1):\n",
    "            mods += [nn.Linear(hidden_dim, hidden_dim), act()]\n",
    "        mods.append(nn.Linear(hidden_dim, output_dim))\n",
    "    if output_mod is not None:\n",
    "        mods.append(output_mod)\n",
    "    trunk = nn.Sequential(*mods)\n",
    "    return trunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp_helper(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, hidden_depth):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_depth = hidden_depth\n",
    "\n",
    "        self.layers = nn.ParameterList([nn.Linear(input_dim, hidden_dim)])\n",
    "        for i in range(hidden_depth - 1):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        def initial(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                std = 1.0 / np.sqrt(m.weight.size(1))\n",
    "                m.weight.data.uniform_(-2.0 * std, 2.0 * std)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        for layer in self.layers:\n",
    "            initial(layer)\n",
    "    def forward(self, x):\n",
    "        act = nn.ReLU()\n",
    "        hidden = act(self.layers[0](x))\n",
    "        saved = hidden\n",
    "        for i in range(1, self.hidden_depth):\n",
    "            hidden = act(self.layers[i](hidden))\n",
    "            if i % 8 == 0:\n",
    "                hidden = hidden + saved\n",
    "                saved = hidden\n",
    "        return self.layers[self.hidden_depth](hidden)\n",
    "        \n",
    "    \n",
    "\n",
    "class SymmetricNeuralODE2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.odefunc = mlp_helper(\n",
    "            input_dim = 6,\n",
    "            hidden_dim = 16,\n",
    "            output_dim = 2,\n",
    "            hidden_depth = 16\n",
    "        )\n",
    "        self.input_weight = torch.Tensor([\n",
    "    [\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1]\n",
    "\n",
    "    ],\n",
    "    [\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0]\n",
    "    ]\n",
    "        ]).float()\n",
    "        self.input_weight2 = torch.Tensor([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "        ]).float()\n",
    "        self.output_weight = torch.Tensor([\n",
    "    [2, 2, -1, -1, -1, -1],\n",
    "    [-1, -1, 2, 2, -1, -1],\n",
    "    [-1, -1, -1, -1, 2, 2]\n",
    "        ]).float()\n",
    "\n",
    "    def forward(self, t, state):\n",
    "        return torch.cat((state[6:], (self.output_weight @ self.odefunc(self.input_weight @ state[:6])).view(-1)))\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def simulate(self, state, times):\n",
    "        solution = odeint(self, state, times, atol=1e-8, rtol=1e-8, method=\"dopri5\", options = {\"dtype\": torch.float32})\n",
    "        # B, S, T\n",
    "        return solution\n",
    "\n",
    "def mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None, act=nn.ReLU):\n",
    "    if hidden_depth == 0:\n",
    "        mods = [nn.Linear(input_dim, output_dim)]\n",
    "    else:\n",
    "        mods = [nn.Linear(input_dim, hidden_dim), act()]\n",
    "        for i in range(hidden_depth - 1):\n",
    "            mods += [nn.Linear(hidden_dim, hidden_dim), act()]\n",
    "        mods.append(nn.Linear(hidden_dim, output_dim))\n",
    "    if output_mod is not None:\n",
    "        mods.append(output_mod)\n",
    "    trunk = nn.Sequential(*mods)\n",
    "    return trunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetricNeuralODE3(nn.Module):\n",
    "    def __init__(self, hidden_dim, hidden_depth):\n",
    "        super().__init__()\n",
    "        self.odefunc = mlp(\n",
    "            input_dim = 6,\n",
    "            hidden_dim = hidden_dim,\n",
    "            output_dim = 6,\n",
    "            hidden_depth = hidden_depth\n",
    "        )\n",
    "        self.input_weight = torch.Tensor([\n",
    "    [\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1]\n",
    "\n",
    "    ],\n",
    "    [\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0]\n",
    "    ]\n",
    "        ]).float()\n",
    "        self.output_weight = torch.Tensor([\n",
    "    [2, 2, -1, -1, -1, -1],\n",
    "    [-1, -1, 2, 2, -1, -1],\n",
    "    [-1, -1, -1, -1, 2, 2]\n",
    "        ]).float()\n",
    "\n",
    "        def init(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                std = 1.0 / np.sqrt(m.weight.size(1))\n",
    "                m.weight.data.uniform_(-2.0 * std, 2.0 * std)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        self.odefunc.apply(init)\n",
    "\n",
    "    def forward(self, t, state):\n",
    "        return torch.cat((state[6:], (self.output_weight @ self.odefunc(self.input_weight @ state[:6])).view(-1)))\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def simulate(self, state, times):\n",
    "        solution = odeint(self, state, times, atol=1e-8, rtol=1e-8, method=\"dopri5\", options = {\"dtype\": torch.float32})\n",
    "        # B, S, T\n",
    "        return solution\n",
    "\n",
    "def mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None, act=nn.ReLU):\n",
    "    if hidden_depth == 0:\n",
    "        mods = [nn.Linear(input_dim, output_dim)]\n",
    "    else:\n",
    "        mods = [nn.Linear(input_dim, hidden_dim), act()]\n",
    "        for i in range(hidden_depth - 1):\n",
    "            mods += [nn.Linear(hidden_dim, hidden_dim), act()]\n",
    "        mods.append(nn.Linear(hidden_dim, output_dim))\n",
    "    if output_mod is not None:\n",
    "        mods.append(output_mod)\n",
    "    trunk = nn.Sequential(*mods)\n",
    "    return trunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperSymmetricNeuralODE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.odefunc = mlp(\n",
    "            input_dim = 3,\n",
    "            hidden_dim = 16,\n",
    "            output_dim = 2,\n",
    "            hidden_depth = 8\n",
    "        )\n",
    "        self.input_weight = torch.Tensor([\n",
    "    [\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1]\n",
    "\n",
    "    ],\n",
    "    [\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0]\n",
    "    ],\n",
    "    [\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0],\n",
    "        [1, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0]\n",
    "    ]\n",
    "        ]).float()\n",
    "        self.input_weight2 = torch.Tensor([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "        ]).float()\n",
    "        self.output_weight = torch.Tensor([\n",
    "    [2, 2, -1, -1, -1, -1],\n",
    "    [-1, -1, 2, 2, -1, -1],\n",
    "    [-1, -1, -1, -1, 2, 2]\n",
    "        ]).float()\n",
    "\n",
    "        def init(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                std = 1.0 / np.sqrt(m.weight.size(1))\n",
    "                m.weight.data.uniform_(-2.0 * std, 2.0 * std)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        self.odefunc.apply(init)\n",
    "\n",
    "    def forward(self, t, state):\n",
    "        # perm11 = state[:6]\n",
    "        # perm12 = perm11[[0, 1, 4, 5, 2, 3]]\n",
    "        # acc1 = self.odefunc(perm11) + self.odefunc(perm12)\n",
    "\n",
    "        # perm21 = perm11[[2, 3, 4, 5, 0, 1]]\n",
    "        # perm22 = perm11[[2, 3, 0, 1, 4, 5]]\n",
    "        # acc2 = self.odefunc(perm21) + self.odefunc(perm22)\n",
    "\n",
    "        # perm31 = perm11[[4, 5, 0, 1, 2, 3]]\n",
    "        # perm32 = perm11[[4, 5, 2, 3, 0, 1]]\n",
    "        # acc3 = self.odefunc(perm31) + self.odefunc(perm32)\n",
    "\n",
    "        # mean = (1/3)*(acc1 + acc2 + acc3)\n",
    "\n",
    "        # acceleration = torch.cat((\n",
    "        #     acc1 - mean,\n",
    "        #     acc2 - mean,\n",
    "        #     acc3 - mean\n",
    "        #     ))\n",
    "        # return torch.cat((state[6:], acceleration))\n",
    "        return torch.cat((state[6:], (self.output_weight @ self.odefunc(self.input_weight @ state[:6])).view(-1)))\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def simulate(self, state, times):\n",
    "        solution = odeint(self, state, times, atol=1e-8, rtol=1e-8, method=\"dopri5\", options = {\"dtype\": torch.float32})\n",
    "        # B, S, T\n",
    "        return solution\n",
    "\n",
    "def mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None, act=nn.ReLU):\n",
    "    if hidden_depth == 0:\n",
    "        mods = [nn.Linear(input_dim, output_dim)]\n",
    "    else:\n",
    "        mods = [nn.Linear(input_dim, hidden_dim), act()]\n",
    "        for i in range(hidden_depth - 1):\n",
    "            mods += [nn.Linear(hidden_dim, hidden_dim), act()]\n",
    "        mods.append(nn.Linear(hidden_dim, output_dim))\n",
    "    if output_mod is not None:\n",
    "        mods.append(output_mod)\n",
    "    trunk = nn.Sequential(*mods)\n",
    "    return trunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6378077268600464\n",
      "1 0.6373531818389893\n",
      "2 0.6372323632240295\n",
      "3 0.6378042101860046\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[238], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m itr \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iter):\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m     trajectory \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpermute(model\u001b[39m.\u001b[39;49msimulate(new_state, obs_times), (\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[1;32m     18\u001b[0m     loss \u001b[39m=\u001b[39m (\n\u001b[1;32m     19\u001b[0m         (trajectory \u001b[39m-\u001b[39m new_orbit)\n\u001b[1;32m     20\u001b[0m         \u001b[39m.\u001b[39mabs()\n\u001b[1;32m     21\u001b[0m         \u001b[39m.\u001b[39mmul(weights)\n\u001b[1;32m     22\u001b[0m         \u001b[39m.\u001b[39mmean()\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m     \u001b[39mif\u001b[39;00m itr \u001b[39m%\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[96], line 103\u001b[0m, in \u001b[0;36mSymmetricNeuralODE.simulate\u001b[0;34m(self, state, times)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimulate\u001b[39m(\u001b[39mself\u001b[39m, state, times):\n\u001b[0;32m--> 103\u001b[0m     solution \u001b[39m=\u001b[39m odeint(\u001b[39mself\u001b[39;49m, state, times, atol\u001b[39m=\u001b[39;49m\u001b[39m1e-8\u001b[39;49m, rtol\u001b[39m=\u001b[39;49m\u001b[39m1e-8\u001b[39;49m, method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdopri5\u001b[39;49m\u001b[39m\"\u001b[39;49m, options \u001b[39m=\u001b[39;49m {\u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m: torch\u001b[39m.\u001b[39;49mfloat32})\n\u001b[1;32m    104\u001b[0m     \u001b[39m# B, S, T\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m solution\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torchdiffeq/_impl/odeint.py:77\u001b[0m, in \u001b[0;36modeint\u001b[0;34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[0m\n\u001b[1;32m     74\u001b[0m solver \u001b[39m=\u001b[39m SOLVERS[method](func\u001b[39m=\u001b[39mfunc, y0\u001b[39m=\u001b[39my0, rtol\u001b[39m=\u001b[39mrtol, atol\u001b[39m=\u001b[39matol, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m event_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     solution \u001b[39m=\u001b[39m solver\u001b[39m.\u001b[39;49mintegrate(t)\n\u001b[1;32m     78\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     event_t, solution \u001b[39m=\u001b[39m solver\u001b[39m.\u001b[39mintegrate_until_event(t[\u001b[39m0\u001b[39m], event_fn)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torchdiffeq/_impl/solvers.py:30\u001b[0m, in \u001b[0;36mAdaptiveStepsizeODESolver.integrate\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_before_integrate(t)\n\u001b[1;32m     29\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(t)):\n\u001b[0;32m---> 30\u001b[0m     solution[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_advance(t[i])\n\u001b[1;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m solution\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torchdiffeq/_impl/rk_common.py:194\u001b[0m, in \u001b[0;36mRKAdaptiveStepsizeODESolver._advance\u001b[0;34m(self, next_t)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mwhile\u001b[39;00m next_t \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrk_state\u001b[39m.\u001b[39mt1:\n\u001b[1;32m    193\u001b[0m     \u001b[39massert\u001b[39;00m n_steps \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_num_steps, \u001b[39m'\u001b[39m\u001b[39mmax_num_steps exceeded (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m>=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(n_steps, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_num_steps)\n\u001b[0;32m--> 194\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrk_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_adaptive_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrk_state)\n\u001b[1;32m    195\u001b[0m     n_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    196\u001b[0m \u001b[39mreturn\u001b[39;00m _interp_evaluate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrk_state\u001b[39m.\u001b[39minterp_coeff, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrk_state\u001b[39m.\u001b[39mt0, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrk_state\u001b[39m.\u001b[39mt1, next_t)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torchdiffeq/_impl/rk_common.py:255\u001b[0m, in \u001b[0;36mRKAdaptiveStepsizeODESolver._adaptive_step\u001b[0;34m(self, rk_state)\u001b[0m\n\u001b[1;32m    250\u001b[0m         dt \u001b[39m=\u001b[39m t1 \u001b[39m-\u001b[39m t0\n\u001b[1;32m    252\u001b[0m \u001b[39m# Must be arranged as doing all the step_t handling, then all the jump_t handling, in case we\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39m# trigger both. (i.e. interleaving them would be wrong.)\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m y1, f1, y1_error, k \u001b[39m=\u001b[39m _runge_kutta_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, y0, f0, t0, dt, t1, tableau\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtableau)\n\u001b[1;32m    256\u001b[0m \u001b[39m# dtypes:\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m# y1.dtype == self.y0.dtype\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m# f1.dtype == self.y0.dtype\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m#                     Error Ratio                      #\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m########################################################\u001b[39;00m\n\u001b[1;32m    265\u001b[0m error_ratio \u001b[39m=\u001b[39m _compute_error_ratio(y1_error, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrtol, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matol, y0, y1, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torchdiffeq/_impl/rk_common.py:76\u001b[0m, in \u001b[0;36m_runge_kutta_step\u001b[0;34m(func, y0, f0, t0, dt, t1, tableau)\u001b[0m\n\u001b[1;32m     74\u001b[0m         perturb \u001b[39m=\u001b[39m Perturb\u001b[39m.\u001b[39mNONE\n\u001b[1;32m     75\u001b[0m     yi \u001b[39m=\u001b[39m y0 \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39msum(k[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m (beta_i \u001b[39m*\u001b[39m dt), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mview_as(f0)\n\u001b[0;32m---> 76\u001b[0m     f \u001b[39m=\u001b[39m func(ti, yi, perturb\u001b[39m=\u001b[39;49mperturb)\n\u001b[1;32m     77\u001b[0m     k \u001b[39m=\u001b[39m _UncheckedAssign\u001b[39m.\u001b[39mapply(k, f, (\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (tableau\u001b[39m.\u001b[39mc_sol[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m (tableau\u001b[39m.\u001b[39mc_sol[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m tableau\u001b[39m.\u001b[39mbeta[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mall()):\n\u001b[1;32m     80\u001b[0m     \u001b[39m# This property (true for Dormand-Prince) lets us save a few FLOPs.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torchdiffeq/_impl/misc.py:189\u001b[0m, in \u001b[0;36m_PerturbFunc.forward\u001b[0;34m(self, t, y, perturb)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39m# Do nothing.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_func(t, y)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[96], line 96\u001b[0m, in \u001b[0;36mSymmetricNeuralODE.forward\u001b[0;34m(self, t, state)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, t, state):\n\u001b[1;32m     76\u001b[0m     \u001b[39m# perm11 = state[:6]\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[39m# perm12 = perm11[[0, 1, 4, 5, 2, 3]]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[39m#     ))\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[39m# return torch.cat((state[6:], acceleration))\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat((state[\u001b[39m6\u001b[39m:], (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_weight \u001b[39m@\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49modefunc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_weight \u001b[39m@\u001b[39;49m state[:\u001b[39m6\u001b[39;49m]))\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state_tensor = torch.Tensor(state).to(device)\n",
    "orbit_tensor = torch.Tensor(orbit).to(device)\n",
    "# model = SymmetricNeuralODE().to(device)\n",
    "model.train()\n",
    "decay = 1.0\n",
    "obs_times = torch.linspace(0, 15, num_steps).to(device)\n",
    "lr = 0.003\n",
    "num_iter = 1000\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "for group in optimizer.param_groups:\n",
    "        group[\"lr\"] = lr\n",
    "weights = 1.0**obs_times\n",
    "\n",
    "\n",
    "for itr in range(num_iter):\n",
    "    optimizer.zero_grad()\n",
    "    trajectory = torch.permute(model.simulate(new_state, obs_times), (1, 0))\n",
    "    loss = (\n",
    "        (trajectory - new_orbit)\n",
    "        .abs()\n",
    "        .mul(weights)\n",
    "        .mean()\n",
    "    )\n",
    "    if itr % 1 == 0:\n",
    "        print(itr, loss.item())\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.9020097255706787\n",
      "Body: 1\n",
      "1 0.4136315882205963\n",
      "2 0.3167489767074585\n",
      "3 0.3459787964820862\n",
      "4 0.4063883423805237\n",
      "5 0.3832593262195587\n",
      "6 0.32524925470352173\n",
      "7 0.3533304035663605\n",
      "8 0.25746241211891174\n",
      "9 0.32121533155441284\n",
      "10 0.335264652967453\n",
      "Body: 0\n",
      "11 0.5442805290222168\n",
      "12 0.37648865580558777\n",
      "13 0.33984869718551636\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[264], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m     current_weight \u001b[39m=\u001b[39m state_weights[i]\n\u001b[1;32m     39\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBody:\u001b[39m\u001b[39m\"\u001b[39m, i)\n\u001b[0;32m---> 40\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     42\u001b[0m optimizer2\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/autograd/function.py:277\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBackwardCFunction\u001b[39;00m(_C\u001b[39m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m    278\u001b[0m         \u001b[39m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         \u001b[39m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         backward_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mbackward  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    281\u001b[0m         vjp_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mvjp  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state_tensor = torch.Tensor(state).to(device)\n",
    "orbit_tensor = torch.Tensor(orbit).to(device)\n",
    "model2 = SymmetricNeuralODE2().to(device)\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "model2.train()\n",
    "decay = 1.0\n",
    "obs_times = torch.linspace(0, 15, num_steps).to(device)\n",
    "lr = 0.01\n",
    "num_iter = 1000\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=lr)\n",
    "for group in optimizer2.param_groups:\n",
    "        group[\"lr\"] = lr\n",
    "weights = 1.0**obs_times\n",
    "on_term = 1\n",
    "off_term = .1\n",
    "state_weights = torch.Tensor([\n",
    "    [on_term, on_term, off_term, off_term, off_term, off_term, on_term, on_term, off_term, off_term, off_term, off_term],\n",
    "    [off_term, off_term, on_term, on_term, off_term, off_term, off_term, off_term, on_term, on_term, off_term, off_term],\n",
    "    [off_term, off_term, off_term, off_term, on_term, on_term, off_term, off_term, off_term, off_term, on_term, on_term]\n",
    "    ]).float().view(3,12,1)\n",
    "current_weight = state_weights[0]\n",
    "\n",
    "\n",
    "for itr in range(num_iter):\n",
    "    optimizer2.zero_grad()\n",
    "    trajectory = torch.permute(model2.simulate(new_state, obs_times), (1, 0))\n",
    "    loss = (\n",
    "        (trajectory - new_orbit)\n",
    "        .abs()\n",
    "        .mul(weights)\n",
    "        .mul(current_weight)\n",
    "        .mean()\n",
    "    )\n",
    "    if itr % 1 == 0:\n",
    "        print(itr, loss.item())\n",
    "    if itr % 10 == 0:\n",
    "        i = np.random.randint(3)\n",
    "        current_weight = state_weights[i]\n",
    "        print(\"Body:\", i)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_tensor = torch.Tensor(state).to(device)\n",
    "orbit_tensor = torch.Tensor(orbit).to(device)\n",
    "model3 = SymmetricNeuralODE2().to(device)\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "model3.train()\n",
    "decay = 1.0\n",
    "obs_times = torch.linspace(0, 15, num_steps).to(device)\n",
    "lr = 0.01\n",
    "num_iter = 1000\n",
    "optimizer3 = torch.optim.Adam(model3.parameters(), lr=lr)\n",
    "for group in optimizer3.param_groups:\n",
    "        group[\"lr\"] = lr\n",
    "weights = 1.0**obs_times\n",
    "on_term = 1\n",
    "off_term = .1\n",
    "state_weights = torch.Tensor([\n",
    "    [on_term, on_term, off_term, off_term, off_term, off_term, on_term, on_term, off_term, off_term, off_term, off_term],\n",
    "    [off_term, off_term, on_term, on_term, off_term, off_term, off_term, off_term, on_term, on_term, off_term, off_term],\n",
    "    [off_term, off_term, off_term, off_term, on_term, on_term, off_term, off_term, off_term, off_term, on_term, on_term]\n",
    "    ]).float().view(3,12,1)\n",
    "current_weight = state_weights[0]\n",
    "\n",
    "\n",
    "for itr in range(num_iter):\n",
    "    optimizer3.zero_grad()\n",
    "    trajectory = torch.permute(model3.simulate(new_state, obs_times), (1, 0))\n",
    "    loss = (\n",
    "        (trajectory - new_orbit)\n",
    "        .abs()\n",
    "        .mul(weights)\n",
    "        .mul(current_weight)\n",
    "        .mean()\n",
    "    )\n",
    "    if itr % 1 == 0:\n",
    "        print(itr, loss.item())\n",
    "    if itr % 10 == 0:\n",
    "        i = np.random.randint(3)\n",
    "        current_weight = state_weights[i]\n",
    "        print(\"Body:\", i)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer3.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# B, S, T\n",
    "#positions = orbit.transpose(2,0,1)[:,:,:2]\n",
    "#predictions = trajectory.detach().cpu().numpy().transpose(2,0,1)[:,:,:2]\n",
    "\n",
    "positions = new_orbit[:6].T.view(num_steps, 3, 2).detach().cpu().numpy()\n",
    "predictions = trajectory[:6].T.view(num_steps, 3, 2).detach().cpu().numpy()\n",
    "\n",
    "file_name = \"symmetric_fixed_orbit\"\n",
    "# Animation\n",
    "# Animation setup\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-30, 30)\n",
    "ax.set_ylim(-30, 30)\n",
    "\n",
    "# Original positions visualized with 'o' markers and solid lines for trajectories\n",
    "lines = [ax.plot([], [], 'o')[0] for _ in range(3)]\n",
    "# Store colors for reuse with approximated trajectories\n",
    "colors = [f\"C{_}\" for _ in range(3)]\n",
    "trajectories = [ax.plot([], [], '-', linewidth=0.5, color=colors[_])[0] for _ in range(3)]\n",
    "\n",
    "# Approximated positions visualized with 'x' markers and dotted lines for trajectories\n",
    "# Use the same colors as the original trajectories\n",
    "lines_approx = [ax.plot([], [], 'x', color=colors[_])[0] for _ in range(3)]\n",
    "trajectories_approx = [ax.plot([], [], '--', linewidth=0.5, color=colors[_])[0] for _ in range(3)]\n",
    "\n",
    "def init():\n",
    "    for line in lines + lines_approx:\n",
    "        line.set_data([], [])\n",
    "    for traj in trajectories + trajectories_approx:\n",
    "        traj.set_data([], [])\n",
    "    return lines + trajectories + lines_approx + trajectories_approx\n",
    "\n",
    "def animate(i):\n",
    "    if len(positions[:i,:,0]) != 0:\n",
    "        all_x = np.concatenate((positions[:i, :, 0].flatten(), predictions[:i, :, 0].flatten()))\n",
    "        all_y = np.concatenate((positions[:i, :, 1].flatten(), predictions[:i, :, 1].flatten()))\n",
    "        min_x, max_x = all_x.min() - 10, all_x.max() + 10\n",
    "        min_y, max_y = all_y.min() - 10, all_y.max() + 10\n",
    "        ax.set_xlim(min_x, max_x)\n",
    "        ax.set_ylim(min_y, max_y)\n",
    "\n",
    "    for j, (line, line_approx) in enumerate(zip(lines, lines_approx)):\n",
    "        # Update original positions and trajectories\n",
    "        line.set_data(positions[i, j, 0], positions[i, j, 1])\n",
    "        trajectories[j].set_data(positions[:i, j, 0], positions[:i, j, 1])\n",
    "        \n",
    "        # Update approximated positions and trajectories\n",
    "        line_approx.set_data(predictions[i, j, 0], predictions[i, j, 1])\n",
    "        trajectories_approx[j].set_data(predictions[:i, j, 0], predictions[:i, j, 1])\n",
    "\n",
    "    return lines + trajectories + lines_approx + trajectories_approx\n",
    "\n",
    "# Assuming num_steps is defined\n",
    "ani = animation.FuncAnimation(fig, animate, frames=range(0, num_steps, 2), init_func=init, blit=True, interval=1)\n",
    "\n",
    "# To save the animation\n",
    "writergif = animation.PillowWriter(fps=50)\n",
    "ani.save(f'{file_name}.gif', writer=writergif)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
